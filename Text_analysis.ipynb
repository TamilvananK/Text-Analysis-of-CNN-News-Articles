{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15461519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china-opposes-tiktok-sale-approval-needed-intl-hnk.txt\n",
      "tiktok-national-security-concerns.txt\n",
      "bytedance-criminal-investigation.txt\n",
      "twitter-verified-checkmarks.txt\n",
      "tiktok-douyin-bytedance-china-intl-hnk.txt\n",
      "tiktok-ban-national-security-hearing.txt\n",
      "south-korea-do-kwon-terra-fraud-arrest-hnk-intl.txt\n",
      "apple-september-event-highlights-trends.txt\n",
      "chevrolet-equinox-ev.txt\n",
      "apple-iphone-14-event.txt\n",
      "tesla-euro-ncap-autopilot.txt\n",
      "eu-combustion-engine-debate-climate-intl.txt\n",
      "china-opposes-tiktok-sale-approval-needed-intl-hnk.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to be scraped\n",
    "urls = ['https://edition.cnn.com/2023/03/24/tech/china-opposes-tiktok-sale-approval-needed-intl-hnk', 'https://edition.cnn.com/2023/03/21/tech/tiktok-national-security-concerns', 'https://edition.cnn.com/2023/03/17/tech/bytedance-criminal-investigation', 'https://edition.cnn.com/2023/03/24/tech/twitter-verified-checkmarks', 'https://edition.cnn.com/2023/03/24/tech/tiktok-douyin-bytedance-china-intl-hnk', 'https://edition.cnn.com/2023/03/24/tech/tiktok-ban-national-security-hearing', 'https://edition.cnn.com/2023/03/23/tech/south-korea-do-kwon-terra-fraud-arrest-hnk-intl', 'https://edition.cnn.com/2022/09/07/tech/apple-september-event-highlights-trends', 'https://edition.cnn.com/2022/09/08/business/chevrolet-equinox-ev', 'https://edition.cnn.com/tech/live-news/apple-iphone-14-event', 'https://edition.cnn.com/2022/09/07/business/tesla-euro-ncap-autopilot', 'https://edition.cnn.com/2023/03/24/cars/eu-combustion-engine-debate-climate-intl', 'https://edition.cnn.com/2023/03/24/tech/china-opposes-tiktok-sale-approval-needed-intl-hnk']\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the article title\n",
    "        title = soup.find('h1').text\n",
    "\n",
    "        # Extract the article text\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.text + '\\n'\n",
    "\n",
    "        # Save the extracted text to a file with URL_ID as its file name\n",
    "        file_path = r'F:\\DATA\\Interview Assignment\\Text Analysis\\Text File'\n",
    "        file_name = url.split('/')[-1]+ '.txt'\n",
    "        file_full_path = os.path.join(file_path, file_name)\n",
    "        with open(file_full_path, 'w', encoding='utf-8') as f:\n",
    "            print(file_name)\n",
    "            #print(title + '\\n\\n' + article_text)\n",
    "            f.write(title + '\\n\\n' + article_text)\n",
    "        f.close()\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the URL.\",url,\" Response code:\",response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "daba5269",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5c6d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple-iphone-14-event.txt\n",
      "Sentiment: Positive\n",
      "{'positive_score': 45, 'negative_score': 17, 'polarity_score': 0.4516128959417275, 'subjectivity_score': 0.03615160347746262}\n",
      "apple-september-event-highlights-trends.txt\n",
      "Sentiment: Neutral\n",
      "{'positive_score': 27, 'negative_score': 20, 'polarity_score': 0.14893616704391135, 'subjectivity_score': 0.030779305808265024}\n",
      "bytedance-criminal-investigation.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 3, 'negative_score': 10, 'polarity_score': -0.5384614970414233, 'subjectivity_score': 0.021885521848677574}\n",
      "chevrolet-equinox-ev.txt\n",
      "Sentiment: Positive\n",
      "{'positive_score': 13, 'negative_score': 4, 'polarity_score': 0.5294117335640156, 'subjectivity_score': 0.01963048496578466}\n",
      "china-opposes-tiktok-sale-approval-needed-intl-hnk.txt\n",
      "Sentiment: Positive\n",
      "{'positive_score': 15, 'negative_score': 12, 'polarity_score': 0.11111110699588492, 'subjectivity_score': 0.03529411760092272}\n",
      "eu-combustion-engine-debate-climate-intl.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 23, 'negative_score': 25, 'polarity_score': -0.041666665798611134, 'subjectivity_score': 0.02782608694039067}\n",
      "south-korea-do-kwon-terra-fraud-arrest-hnk-intl.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 2, 'negative_score': 15, 'polarity_score': -0.7647058373702448, 'subjectivity_score': 0.026729559706400063}\n",
      "tesla-euro-ncap-autopilot.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 19, 'negative_score': 22, 'polarity_score': -0.07317072992266513, 'subjectivity_score': 0.044565217342863894}\n",
      "tiktok-ban-national-security-hearing.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 55, 'negative_score': 70, 'polarity_score': -0.11999999904000001, 'subjectivity_score': 0.04155585105001468}\n",
      "tiktok-douyin-bytedance-china-intl-hnk.txt\n",
      "Sentiment: Positive\n",
      "{'positive_score': 27, 'negative_score': 17, 'polarity_score': 0.22727272210743815, 'subjectivity_score': 0.030199039100755633}\n",
      "tiktok-national-security-concerns.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 48, 'negative_score': 56, 'polarity_score': -0.07692307618343196, 'subjectivity_score': 0.040977147343980634}\n",
      "twitter-verified-checkmarks.txt\n",
      "Sentiment: Negative\n",
      "{'positive_score': 4, 'negative_score': 10, 'polarity_score': -0.42857139795918586, 'subjectivity_score': 0.017412935301725205}\n",
      "                                            file_name  positive_score  \\\n",
      "0                           apple-iphone-14-event.txt              45   \n",
      "1         apple-september-event-highlights-trends.txt              27   \n",
      "2                bytedance-criminal-investigation.txt               3   \n",
      "3                            chevrolet-equinox-ev.txt              13   \n",
      "4   china-opposes-tiktok-sale-approval-needed-intl...              15   \n",
      "5        eu-combustion-engine-debate-climate-intl.txt              23   \n",
      "6   south-korea-do-kwon-terra-fraud-arrest-hnk-int...               2   \n",
      "7                       tesla-euro-ncap-autopilot.txt              19   \n",
      "8            tiktok-ban-national-security-hearing.txt              55   \n",
      "9          tiktok-douyin-bytedance-china-intl-hnk.txt              27   \n",
      "10              tiktok-national-security-concerns.txt              48   \n",
      "11                    twitter-verified-checkmarks.txt               4   \n",
      "\n",
      "    negative_score  polarity_score  subjectivity_score  avg_sentence_length  \\\n",
      "0               17        0.451613            0.036152            23.819444   \n",
      "1               20        0.148936            0.030779            20.917808   \n",
      "2               10       -0.538461            0.021886            18.562500   \n",
      "3                4        0.529412            0.019630            19.244444   \n",
      "4               12        0.111111            0.035294            20.131579   \n",
      "5               25       -0.041667            0.027826            20.535714   \n",
      "6               15       -0.764706            0.026730            19.272727   \n",
      "7               22       -0.073171            0.044565            18.775510   \n",
      "8               70       -0.120000            0.041556            27.099099   \n",
      "9               17        0.227273            0.030199            17.141176   \n",
      "10              56       -0.076923            0.040977            27.586957   \n",
      "11              10       -0.428571            0.017413            19.609756   \n",
      "\n",
      "    percentage_complex_words  fog_index  avg_words_per_sentence  \\\n",
      "0                   0.077551   9.558798               25.597015   \n",
      "1                   0.077276   8.398034               23.492308   \n",
      "2                   0.122896   7.474158               20.482759   \n",
      "3                   0.103926   7.739348               21.121951   \n",
      "4                   0.101961   8.093416               22.500000   \n",
      "5                   0.120580   8.262518               23.310811   \n",
      "6                   0.106918   7.751858               21.931034   \n",
      "7                   0.135870   7.564552               20.909091   \n",
      "8                   0.129322  10.891368               30.080000   \n",
      "9                   0.085793   6.890788               19.689189   \n",
      "10                  0.134358  11.088526               30.951220   \n",
      "11                  0.085821   7.878231               21.729730   \n",
      "\n",
      "    complex_word_count  word_count  syllable_count_per_word  \\\n",
      "0                  133        1715                 1.392420   \n",
      "1                  118        1527                 1.384414   \n",
      "2                   73         594                 1.506734   \n",
      "3                   90         866                 1.408776   \n",
      "4                   78         765                 1.460131   \n",
      "5                  208        1725                 1.464928   \n",
      "6                   68         636                 1.435535   \n",
      "7                  125         920                 1.560870   \n",
      "8                  389        3008                 1.518617   \n",
      "9                  125        1457                 1.438572   \n",
      "10                 341        2538                 1.533097   \n",
      "11                  69         804                 1.436567   \n",
      "\n",
      "    personal_pronouns  avg_word_len  \n",
      "0                   2      4.193003  \n",
      "1                   4      4.218075  \n",
      "2                   9      4.563973  \n",
      "3                   1      4.290993  \n",
      "4                   5      4.427451  \n",
      "5                   3      4.411594  \n",
      "6                   1      4.333333  \n",
      "7                   2      4.680435  \n",
      "8                  28      4.482380  \n",
      "9                   9      4.330130  \n",
      "10                 23      4.527975  \n",
      "11                  2      4.378109  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load words from a file and return as a list\n",
    "def load_words(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        words = list(file.read().splitlines())\n",
    "    return words\n",
    "\n",
    "# Function to calculate sentiment of the text\n",
    "def calculate_sentiment(clean_text, stop_words, positive_words, negative_words):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    for word in clean_text:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        elif word in positive_words:\n",
    "            positive_count += 1\n",
    "        elif word in negative_words:\n",
    "            negative_count += 1\n",
    "    if positive_count > negative_count:\n",
    "        return \"Positive\"\n",
    "    elif positive_count < negative_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Function to perform sentimental analysis on the text\n",
    "def sentimental_analysis(words, positive_words, negative_words):\n",
    "\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        total_words = len(words)\n",
    "        # calculate positive and negative scores\n",
    "        for word in words:\n",
    "            if word in positive_words:\n",
    "                positive_score += 1\n",
    "            elif word in negative_words:\n",
    "                negative_score += 1\n",
    "\n",
    "        # calculate polarity score\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "        # calculate subjectivity score\n",
    "        subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "\n",
    "        return {'positive_score' : positive_score,'negative_score' : negative_score, \n",
    "                'polarity_score' : polarity_score, 'subjectivity_score' : subjectivity_score} \n",
    "\n",
    "stop_words = load_words(\"stop_words.txt\")\n",
    "positive_words = load_words(\"positive-words.txt\")\n",
    "negative_words = load_words(\"negative-words.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform readability analysis on the text\n",
    "def readability_analysis(text_words,article):\n",
    "    words = text_words \n",
    "    num_words = len(words)\n",
    "    #print(num_words)\n",
    "    num_sentences = article.count(\".\") + article.count(\"!\") + article.count(\"?\")\n",
    "    #print(num_sentences)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    \n",
    "    # Function to perform syllable in word\n",
    "    def syllables_in_word(word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiou\"\n",
    "        count = 0\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            count -= 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    # Count the number of complex words in the text\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        if syllables_in_word(word) > 2:\n",
    "            complex_word_count += 1\n",
    "            \n",
    "    # Calculate the percentage of complex words       \n",
    "    percentage_complex_words = complex_word_count / num_words\n",
    "    \n",
    "    # Calculate the average number of syllables per word\n",
    "    syllable_count_per_word = sum([syllables_in_word(word) for word in words]) / len(words)\n",
    "    \n",
    "    # Count the number of personal pronouns in the text\n",
    "    personal_pronouns = 0\n",
    "    personal_pronouns_list = ['I', 'we', 'my', 'ours', 'us']\n",
    "    for word in words:\n",
    "        if word.lower() in personal_pronouns_list:\n",
    "            personal_pronouns += 1\n",
    "    \n",
    "    # Calculate the average word length\n",
    "    total_chars = 0\n",
    "    for word in words:\n",
    "        total_chars += len(word)\n",
    "    avg_word_len = total_chars / len(words)\n",
    "    \n",
    "    # Function to calculate average number of words per sentence\n",
    "    tokens = tokenize_text\n",
    "    sentences = nltk.sent_tokenize(Article_text)\n",
    "    num_words = len(tokens)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    \n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    return {'avg_sentence_length' : avg_sentence_length,\n",
    "            'percentage_complex_words' : percentage_complex_words,\n",
    "            'fog_index' : fog_index,\n",
    "            'avg_words_per_sentence' : avg_words_per_sentence,\n",
    "            'complex_word_count' : complex_word_count,\n",
    "            'word_count' : len(tokenize_text),\n",
    "            'syllable_count_per_word' : syllable_count_per_word,\n",
    "            'personal_pronouns' : personal_pronouns,\n",
    "            'avg_word_len' : avg_word_len}\n",
    "            \n",
    "# Directory containing the .txt files\n",
    "directory = r'F:\\DATA\\Interview Assignment\\Text Analysis\\Text File\\text_file'\n",
    "\n",
    "# Create a list to store the analysis data\n",
    "analysis_data = []\n",
    "\n",
    "# Loop through all the .txt files in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        print(file_name)\n",
    "        # Open the .txt file and perform the analysis\n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            Article_text = f.read()\n",
    "            \n",
    "            # Import text from text file\n",
    "            #Article_text = open(\"man-and-machines-together-machines-are-more-diligent-than-humans.txt\", encoding=\"utf-8\").read()\n",
    "            ###print(article_text)\n",
    "            \n",
    "            #clean the text file\n",
    "            ###clean_text = text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "\n",
    "            #Tokenize text using word_tokenizer\n",
    "            tokenize_text = word_tokenize(Article_text)\n",
    "\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            clean_text = [word.lower() for word in tokenize_text if word not in string.punctuation]\n",
    "\n",
    "            # Calculate sentiment of the text\n",
    "            sentiment = calculate_sentiment(tokenize_text, stop_words, positive_words, negative_words)\n",
    "            print(\"Sentiment:\", sentiment)\n",
    "            \n",
    "            # Perform the sentiment analysis\n",
    "            sentiment_analysis_out = sentimental_analysis(tokenize_text, positive_words, negative_words)\n",
    "            print(sentiment_analysis_out)\n",
    "            \n",
    "            # Perform the readability analysis\n",
    "            readability_analysis_out = readability_analysis(tokenize_text, Article_text)\n",
    "            \n",
    "            # Combine the analysis data and append to the list\n",
    "            analysis = {'file_name': file_name,**sentiment_analysis_out, **readability_analysis_out}\n",
    "            analysis_data.append(analysis)\n",
    "\n",
    "# Convert the analysis data list to a pandas DataFrame\n",
    "df = pd.DataFrame(analysis_data)\n",
    "print(df)\n",
    "# Save the DataFrame to an Excel file\n",
    "file_path = r'F:\\DATA\\Interview Assignment\\Text Analysis\\Output_Data.xlsx'\n",
    "df.to_excel(file_path, sheet_name=\"sheet3\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3c6df08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86164889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
